spec:
  containers:
    - name: llm-container
      image: /*/code_schema/demo_repo/llm
      env:
        HF_TOKEN: <<HF_TOKEN>>
        NUM_GPU: 1 # if running in GPU 7 or GPU 10, update accordingly
        MAX_GPU_MEMORY: 24Gib # if running in GPU 7 or GPU 10, update accordingly
      volumeMounts:
        - name: config
          mountPath: /snowflake_config
        - name: models
          mountPath: /models
    - name: udf
      image: /*/code_schema/demo_repo/udf-flask
      env:
        OPENAI_API_BASE: http://localhost:8000/v1
        MODEL: Llama-2-7b-chat-hf
  endpoints:
    - name: chat
      port: 5000
      public: false
    - name: llm
      port: 8000
      public: true
  volumes:
    - name: config
      source: "@app_internal.config"
    - name: models
      source: "@app_public.models"
  networkPolicyConfig:
    allowInternetEgress: true
